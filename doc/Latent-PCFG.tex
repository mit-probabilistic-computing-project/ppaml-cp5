\documentclass[english]{article}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage[USenglish]{babel}
\usepackage{verbatim}
\usepackage[numbers]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref} 

\makeatletter

\usepackage{hyperref}

\RequirePackage{colortbl, tabularx}

% Just a little trick to make comments visible for the draft version.
\@ifundefined{comment}{}% do nothing if the comment environment is not defined
  {% redefine the comment environment if it is defined
   \renewenvironment{comment}
    {% replaces \begin{comment}
     \par\medskip\noindent
     \tabularx{\textwidth}{|>{\columncolor[gray]{0.9}}X|}
     \hline
     \emph{\textbf{Comment:}}% You can use any other text instead of "Comment:" or leave it
    }
    {% replaces \end{comment}
     \endtabularx\hrule\par\medskip
    }
  }%

\makeatother

\begin{document}

\title{Challenge Problem 5\\
Probabilistic Context-Free Grammars with Latent Annotation}

\author{Chad Scherrer, Tom Dietterich}

\date{\today}

\maketitle
% \tableofcontents{}

\section{Introduction}
Context-free grammars (CFGs) provide a simple model for the structure of language and are widely-applied in natural language processing systems. A CFG consists of a set of non-terminal symbols $N$, a set of terminal symbols $T$, and designated start symbol $S$, and a set of production rules of the form $N \rightarrow R$, where $R$ is a sequence of terminals or non-terminals. For each nonterminal $n \in N$, the set of rules having $n$ on their left-hand-side are the ``rules for $n$'', which we will denote as $Rules(n)$.  A sentence is generated by beginning with the start symbol $S$, choosing one of the rewrite rules in $Rules(S)$ and replacing $S$ by the right-hand-side of the chosen rule. This is repeated, each time expanding one of the non-terminals in the emerging sentence, until no non-terminals remain.

Probabilistic context-free grammars (PCFGs) extend CFGs to define a probability distribution over the sentences and parse-trees generated by the grammar by specifying a separate multinomial distribution for each non-terminal $n$ over $Rules(n)$.  These probability distributions can be learned from data consisting of sentences and their parse trees (``treebanks'').  The most famous treebank is the Wall Street Journal (WSJ) treebank developed at the University of Pennsylvania. It consists of 23 ``sections'', and it is available at no cost from the Linguistic Data Consortium (LDC) at the University of Pennsylvania. The standard approach to fitting PCFGs is to apply EM and use the Inside-Outside algorithm to perform the E-step \cite{baker1979,collins:io}. Parsing is performed via the CYK algorithm \cite{Cocke1970}. 

A weakness of both CFGs and PCFGs is that each time a non-terminal is expanded using one of its rules, the choice is made independently of all other choices. This independence prevents PCFGs from capturing many important linguistic regularities. One way to address this problem is to replace very general non-terminals such as NP (noun phrase) and N (noun) with an expanded set of symbols.  For example, we could have NP-animate, NP-inanimate, N-animate, and N-inanimate, to separately model noun phrases and nouns referring to animate versus inanimate objects. These are sometimes referred to as annotations, because one can imagine manually annotating each N and NP in a treebank with this additional information.  However, there are no large annotated treebanks, and it is also not clear what annotations should be introduced.

One way to avoid manual annotation is to take the extreme approach known as lexicalization in which a separate ``annotation'' is defined for each word in the lexicon. For example, we would have N-car, N-truck, N-bus and so on.  This leads to an immense grammar, and learning the parameters for the probability distributions requires introducing some form of ``smoothing'' so that rules for similar words are given similar probabilities.

In this challenge problem, we will investigate a different approach pioneered by \citet{Matsuzaki2005} in which the annotations are ``latent''.  That is, we could view the parse trees in the treebank as having ``missing'' annotations.  We specify that each non-terminal can have up to $k$ annotations, and it is the job of the learning algorithm to determine what those annotations should be and which sentences should use which annotations.  In statistical modeling terms, we replace each nonterminal by a mixture of distributions over its child non-terminals (which are in turn, recursively, mixtures over their children).  Recent work on this model includes \citet{Petrov2006} and \citet{Cohen2012}.  

\section{Problem Overview}

This problem will be given in two stages.  
\begin{description}
\item[Stage 1:] Fit a standard PCFG to a subset of the WSJ corpus.
\item[Stage 2:] Fit a latent PCFG to a subset of the WSJ corpus. The number of latent annotations will be fixed.
\item[Stage 2 HDP option:] Fit a latent PCFG to a subset of the WSJ corpus. The number of latent annotations will be flexible and modeled via a Hierarchical Dirichlet Process.
\end{description}

Training data will consist of a text corpus (from the WSJ section of OntoNotes 5.0) with associated constituency-based parse trees (\emph{i.e.}, one terminal associated with each nonterminal leaf node). The trees will be binarized, which is a standard transformation intended to simplify the parsing code. Galois will also supply a small synthetic data set for program development and testing.

A good introduction to the field that only assumes general statistical modeling expertise can be found in the introduction section of \citet{Liang2009}.

\section{Probability model}

The following subsections describe iterations of the models to be expressed. Where appropriate, particular algorithms are mentioned. This is intended to be informative for the teams, but \textbf{there is no requirement to implement these particular algorithms.} The primary focus is on expressing the model in a natural way.
 
\subsection{July 2015: Probabilistic Context-Free Grammar}

The initial challenge will be a probabilistic context-free grammar (PCFG) to a training set and evaluate performance on a test set. The PCFG model is most commonly trained using the \emph{inside-outside algorithm} (a form of EM). Once the production probabilities are estimated, parsing is usually performed using the \emph{CYK algorithm}. More details on PCFGs are available from \citet{Collins}.

\subsection{January 2016: PCFG with latent annotations}

The PCFG-LA model is an extension of PCFG in which each nonterminal is assumed to have a latent annotation. Thus, a particular node in the training data might be labeled as NN (noun), but the model assumes this is realized as an instance of one of NN$_{1}$ through NN$_{K}$ for some fixed $K$. After training the model, these latent subcategories are often seen to correspond to clusters with some semantic interpretation. For example, NN$_{1}$ might consist primarily of objects that can be picked up (and hence correlate with verbs such as ``pickup''), while NN$_{2}$ might consist of types of animals (and hence correlate with verbs such as ``see'', ``catch'', or ``feed'').

Details of the model definition can be found in \citet{Matsuzaki2005}, and an excellent brief overview has been produced by \citet{Manning2012}.

The PCFG-LA model can be trained using the EM algorithm, but this depends strongly on initialization due to local optima in the posterior density. Matsuzaki et al. handle initialization by providing annotations of the identity of the parent and sibling. 

\citet{Petrov2006} instead starts with a minimal grammar and extends it with new latent subcategories using a split/merge heuristic. 

A stretch goal, for ambitious teams, will be to replace the assumption of fixed $K$ with a Hierarchical Dirichlet Process (HDP) prior. This is not required, but Galois will evaluate implementations that are submitted. For previous work in this direction, see \citet{Liang2007, Liang2009}.

\subsubsection*{Prediction}

The prediction problem is to assign to a given sentence a parse tree (or distribution over parse trees, in a fully Bayesian approach). Unfortunately, as Matsuzaki et al. point out, the problem of optimizing the posterior density of a PCFG-LA model is NP-hard. Matsuzaki and Petrov thus both rely on heuristics for this computation; see the respective publications for details.

\subsection{Opportunities for Probabilistic Programming}

Existing implementations of these models are very complex software systems. For example, Petrov's system requires 41240 lines of code (including comments). Probabilistic Programming languages hold the promise of making it much easier to write these programs. A second opportunity is to demonstrate that different probabilistic model components (finite mixtures, HDP mixtures) can be implemented as modular components and combined flexibly.

\section{Training and test data}

For training and test data, we plan to use Ontonotes 5.0, which is available at no charge from \citet{LDC2013}. From LDC's description: 
\begin{quote}
The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).
\end{quote}

The English portion of OntoNotes consists of 1.4 million words from a variety of sources (News, BN, BC, Web, Tele, and Pivot).

The directory \texttt{ontonotes-release-5.0/data/files/data/english/annotations/nw/wsj} contains directories 00 through 24. Within each of these directories you'll find (among other files) some files with names of the form \texttt{wsj\_*.parse}. These \texttt{.parse} files contain the parse trees. 

Petrov describes the ``standard setup'' for evaluation:
\begin{quote}
We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the standard setup: we trained on sections 2 to 21, and we used section 1 as a validation set for tuning model hyperparameters. Section 22 was used as development set for intermediate results. All of section 23 was reserved for the final test. We used the EVALB parseval reference implementation, available from Sekine and Collins (1997), for scoring. All reported development set results are averages over four runs. For the final test we selected the grammar that performed best on the development set.
\end{quote}

We will follow the standard setup as described by Petrov, but will train and test on smaller subsets of the data.  Galois will preprocess the parse trees to ``binarize'' them (so that every production has only 2 symbols on its right-hand-side). This simplifies the design of various algorithms (CYK, Inside-Outside, etc.).

\section{Evaluation}

\subsection{Quantitative evaluation}

Quantitative evaluation will be in terms of the following metrics:
\begin{enumerate}
\item Program size (lines of code)
\item Computational cost: execution time and memory
\item Parsing accuracy: the standard $F_{1}$ metric, as computed using the Evalb program of \citet{Sekine1997}
\end{enumerate}

In the context of parsing, \emph{precision} is defined as the proportion of predicted nodes (over all levels of the parse tree) that are also correctly parsed, and \emph{recall} is the proportion of true nodes that are correctly parsed. The $F_{1}$ metric is then computed as the harmonic mean of precision and recall.

The Berkeley parser of \citet{Petrov}, with identically-binarized training data, will be used as a gold standard. It is understood that this parser is hand-tuned code that is the result of a substantial development effort. The challenge is to approach the performance of this state-of-the-art parser with some very small fraction of the development effort.

\subsection{Qualitative evaluation}

In addition to the usual considerations, a \texttt{diff} of the PCFG and PCFG-LA submissions should reveal strong similarities, with modifications in the code corresponding as closely as possible to the conceptual modification to the model.

It is also desirable to have a clean separation between the learning algorithm (e.g., EM) and the model and between the HDP and the remainder of the model (if that option is attempted).

\bibliographystyle{plainnat}
\phantomsection\addcontentsline{toc}{section}{\refname}\bibliography{Latent-PCFG}
 
\end{document}
