@article{Cohen2012,
author = {Cohen, Shay B and Stratos, Karl and Collins, Michael and Foster, Dean P and Ungar, Lyle},
file = {:home/chad/Dropbox/Mendeley//Cohen et al. - 2012 - Experiments with Spectral Learning of Latent-Variable PCFGs.pdf:pdf},
title = {{Experiments with Spectral Learning of Latent-Variable PCFGs}},
year = {2012}
}
@article{Cohen,
author = {Cohen, Shay B and Stratos, Karl and Collins, Michael and Foster, Dean P and Ungar, Lyle},
file = {:home/chad/Dropbox/Mendeley//Cohen et al. - Unknown - Spectral Learning of Latent-Variable PCFGs.pdf:pdf},
number = {2009},
pages = {1--31},
title = {{Spectral Learning of Latent-Variable PCFGs}}
}
@article{Doucet2008,
author = {Doucet, Arnaud and Arnaudismacjp, Email and Johansen, Adam M},
file = {:home/chad/Dropbox/Mendeley/Doucet, Arnaudismacjp, Johansen - 2008 - A Tutorial on Particle Filtering and Smoothing Fifteen years later.pdf:pdf},
number = {December},
pages = {4--6},
title = {{A Tutorial on Particle Filtering and Smoothing : Fifteen years later}},
year = {2008}
}
@article{Huang2009,
author = {Huang, Zhongqiang},
file = {:home/chad/Dropbox/Mendeley/Huang - 2009 - Self-Training PCFG Grammars with Latent Annotations Across Languages.pdf:pdf},
number = {August},
pages = {832--841},
title = {{Self-Training PCFG Grammars with Latent Annotations Across Languages}},
year = {2009}
}
@inproceedings{Huang2011,
author = {Huang, Zhongqiang and Harper, Mary},
booktitle = {International Joint Conference on Natural Language Processing},
file = {:home/chad/Dropbox/Mendeley/Huang, Harper - 2011 - Feature-Rich Log-Linear Lexical Model for Latent Variable PCFG Grammars.pdf:pdf},
pages = {219--227},
title = {{Feature-Rich Log-Linear Lexical Model for Latent Variable PCFG Grammars}},
year = {2011}
}
@article{Huang2011a,
author = {Huang, Zhongqiang and Harper, Mary},
file = {:home/chad/Dropbox/Mendeley/Huang, Harper - 2011 - Feature-Rich Log-Linear Lexical Model for Latent Variable PCFG Grammars.pdf:pdf},
pages = {219--227},
title = {{Feature-Rich Log-Linear Lexical Model for Latent Variable PCFG Grammars}},
year = {2011}
}
@article{Huang,
author = {Huang, Zhongqiang and Harper, Mary},
file = {:home/chad/Dropbox/Mendeley/Huang, Harper, Petrov - 2010 - Self-training with Products of Latent Variable Grammars University of Maryland.pdf:pdf},
title = {{Self-training with Products of Latent Variable Grammars University of Maryland}}
}
@inproceedings{Huang2010,
author = {Huang, Zhongqiang and Harper, Mary and Petrov, Slav},
booktitle = {EMNLP},
file = {:home/chad/Dropbox/Mendeley/Huang, Harper, Petrov - 2010 - Self-training with Products of Latent Variable Grammars University of Maryland.pdf:pdf},
title = {{Self-training with Products of Latent Variable Grammars University of Maryland}},
year = {2010}
}
@article{Kiselyov2004,
address = {New York, New York, USA},
author = {Kiselyov, Oleg and L\"{a}mmel, Ralf and Schupke, Keean},
doi = {10.1145/1017472.1017488},
file = {:home/chad/Dropbox/Mendeley//Kiselyov, L\"{a}mmel, Schupke - 2004 - Strongly typed heterogeneous collections.pdf:pdf},
isbn = {1581138504},
journal = {Proceedings of the ACM SIGPLAN workshop on Haskell - Haskell '04},
keywords = {cess,collections,dependently typed programming,equality,extensible records,haskell,type,type improvement,type-indexed rows,type-safe database ac-},
pages = {96},
publisher = {ACM Press},
title = {{Strongly typed heterogeneous collections}},
url = {http://portal.acm.org/citation.cfm?doid=1017472.1017488},
year = {2004}
}
@incollection{Liang2009,
abstract = {Probabilistic context-free grammars (PCFGs) have played an important role in the model- ing of syntax in natural language processing and other applications, but choosing the proper model complexity is often difficult. We present a nonparametric Bayesian generalization of the PCFG based on the hierarchical Dirichlet process (HDP). In our HDP-PCFG model, the effective complexity of the grammar can grow with increasing data. We describe an efficient variational inference algorithm for our model and present experiments on both a synthetic grammar induc- tion task and a large-scale natural language parsing task. Keywords:},
author = {Liang, Percy and Jordan, Michael I and Klein, Dan},
booktitle = {The Handbook of Applied Bayesian Analysis},
editor = {O'Hagan, Tony and West, Mike},
file = {:home/chad/Dropbox/Mendeley/Liang, Jordan, Klein - 2009 - Probabilistic Grammars and Hierarchical Dirichlet Processes.pdf:pdf},
keywords = {natural language processing,nonparametric bayesian statistics,variational inference},
publisher = {Oxford University Press},
title = {{Probabilistic Grammars and Hierarchical Dirichlet Processes}},
url = {http://www.cs.berkeley.edu/~jordan/papers/liang-jordan-klein-haba.pdf},
year = {2009}
}
@inproceedings{Liang2007,
author = {Liang, Percy and Petrov, Slav and Jordan, Michael I and Klein, Dan},
booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
file = {:home/chad/Dropbox/Mendeley/Liang et al. - 2007 - The Infinite PCFG using Hierarchical Dirichlet Processes.pdf:pdf},
number = {June},
pages = {688--697},
title = {{The Infinite PCFG using Hierarchical Dirichlet Processes}},
year = {2007}
}
@misc{LDC2013,
author = {{Linguistic Data Consortium}},
title = {{OntoNotes Release 5.0}},
url = {https://catalog.ldc.upenn.edu/LDC2013T19},
year = {2013}
}
@misc{Manning2012,
author = {Manning, Christopher},
booktitle = {Natural Language Processing},
publisher = {Stanford Coursera},
title = {{Latent Variable PCFGs}},
url = {https://class.coursera.org/nlp/lecture/174},
year = {2012}
}
@misc{Manning2012a,
author = {Manning, Christopher},
title = {{Constituency Parser Evaluation}},
url = {https://class.coursera.org/nlp/lecture/169},
year = {2012}
}
@article{Mansinghka2014,
abstract = {We describe Venture, an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive, extensible, and efficient for general-purpose use. Like Church, probabilistic models and inference problems in Venture are specified via a Turing-complete, higher-order probabilistic language descended from Lisp. Unlike Church, Venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques. We also describe four key aspects of Venture's implementation that build on ideas from probabilistic graphical models. First, we describe the stochastic procedure interface (SPI) that specifies and encapsulates primitive random variables. The SPI supports custom control flow, higher-order probabilistic procedures, partially exchangeable sequences and ``likelihood-free'' stochastic simulators. It also supports external models that do inference over latent variables hidden from Venture. Second, we describe probabilistic execution traces (PETs), which represent execution histories of Venture programs. PETs capture conditional dependencies, existential dependencies and exchangeable coupling. Third, we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems. Finally, we describe a family of stochastic regeneration algorithms for efficiently modifying PET fragments contained within scaffolds. Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically. We show how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques.},
archivePrefix = {arXiv},
arxivId = {1404.0099},
author = {Mansinghka, Vikash and Selsam, Daniel and Perov, Yura},
eprint = {1404.0099},
file = {:home/chad/Dropbox/Mendeley//Mansinghka, Selsam, Perov - 2014 - Venture a higher-order probabilistic programming platform with programmable inference.pdf:pdf},
keywords = {bayesian inference,bayesian networks,markov chain,monte carlo,particle markov chain monte,probabilistic programming,sequential monte carlo,variational inference},
month = mar,
pages = {78},
title = {{Venture: a higher-order probabilistic programming platform with programmable inference}},
url = {http://arxiv.org/abs/1404.0099},
year = {2014}
}
@article{Matsuzaki2005,
abstract = {This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6percent (F1, sentences le 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.},
author = {Matsuzaki, Takuya and Miyao, Yusuke and Tsujii, Jun'ichi},
doi = {doi:10.3115/1219840.1219850},
file = {:home/chad/Dropbox/Mendeley//Matsuzaki, Miyao, Tsujii - 2005 - Probabilistic CFG with latent annotations.pdf:pdf},
isbn = {1932432515},
journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ACL 05},
pages = {75--82},
title = {{Probabilistic CFG with latent annotations}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219850},
volume = {05pages},
year = {2005}
}
@article{Milch,
author = {Milch, Brian and Russell, Stuart},
file = {:home/chad/Dropbox/Mendeley//Milch, Russell - Unknown - BLOG Probabilistic Models with Unknown Objects.pdf:pdf},
title = {{BLOG : Probabilistic Models with Unknown Objects}}
}
@inproceedings{Petrov2006,
author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL},
file = {:home/chad/Dropbox/Mendeley//Petrov et al. - 2006 - Learning Accurate , Compact , and Interpretable Tree Annotation.pdf:pdf},
number = {July},
pages = {433--440},
title = {{Learning Accurate, Compact, and Interpretable Tree Annotation}},
year = {2006}
}
@article{Pfeffer,
author = {Pfeffer, Avi},
file = {:home/chad/Dropbox/Mendeley//Pfeffer - Unknown - Figaro An Object-Oriented Probabilistic Programming Language.pdf:pdf},
pages = {1--9},
title = {{Figaro : An Object-Oriented Probabilistic Programming Language}}
}
@article{Reis2005,
author = {Reis, D. S. and Stedinger, J. R. and Martins, E. S.},
doi = {10.1029/2004WR003445},
file = {:home/chad/Dropbox/Mendeley/Reis, Stedinger, Martins - 2005 - Bayesian generalized least squares regression with application to log Pearson type 3 regional skew est.pdf:pdf},
issn = {00431397},
journal = {Water Resources Research},
keywords = {http://dx.doi.org/10.1029/2004WR003445, doi:10.102},
month = oct,
number = {10},
pages = {n/a--n/a},
title = {{Bayesian generalized least squares regression with application to log Pearson type 3 regional skew estimation}},
url = {http://doi.wiley.com/10.1029/2004WR003445},
volume = {41},
year = {2005}
}
@article{Seeger,
author = {Seeger, Matthias and Gerwinn, Sebastian and Bethge, Matthias},
file = {:home/chad/Dropbox/Mendeley/Seeger, Gerwinn, Bethge - Unknown - Bayesian Inference for Sparse Generalized Linear Models.pdf:pdf},
title = {{Bayesian Inference for Sparse Generalized Linear Models}}
}
@misc{Sekine1997,
author = {Sekine, Satoshi and Collins, Michael},
title = {{Evalb - Bracket scoring program}},
url = {http://nlp.cs.nyu.edu/evalb/},
year = {1997}
}
@article{Sorensen2014,
author = {Sorensen, Tanner},
file = {:home/chad/Dropbox/Mendeley/Sorensen - 2014 - Fitting linear mixed models using JAGS and Stan A tutorial.pdf:pdf},
keywords = {bayesian linear mixed models,jags,stan},
title = {{Fitting linear mixed models using JAGS and Stan: A tutorial}},
year = {2014}
}
@article{Wingate2013,
abstract = {We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.1299v1},
author = {Wingate, David and Weber, Theo},
eprint = {arXiv:1301.1299v1},
file = {:home/chad/Dropbox/Mendeley//Wingate, Weber - 2013 - Automated variational inference in probabilistic programming.pdf:pdf},
journal = {arXiv preprint arXiv:1301.1299},
pages = {1--7},
title = {{Automated variational inference in probabilistic programming}},
url = {http://arxiv.org/abs/1301.1299},
year = {2013}
}
